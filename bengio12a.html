<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
<!--#include virtual="/css-scroll.txt"-->
  <title>Deep Learning of Representations for Unsupervised and Transfer
Learning</title>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
  <meta name="generator"
 content="TeX4ht (http://www.cse.ohio-state.edu/~gurari/TeX4ht/)">
  <meta name="originator"
 content="TeX4ht (http://www.cse.ohio-state.edu/~gurari/TeX4ht/)">
<!-- uni-html4,html -->
  <meta name="src" content="bengio12a.tex">
  <meta name="date" content="2012-05-27 11:11:00">
  <link rel="stylesheet" type="text/css" href="bengio12a.css">
</head>
<body>
<!--l. 123-->
<div id="content">
<div class="maketitle">
<h2 class="titleHead">Deep Learning of Representations for Unsupervised
and Transfer Learning</h2>
</div>
<span class="cmbxti-10x-x-109">Y. Bengio</span>; JMLR W&amp;CP
27:17–36, 2012.
<div class="abstract"><!--l. 135-->
<p class="indent"> </p>
<h3>Abstract</h3>
Deep learning algorithms seek to exploit the unknown structure in the
input
distribution in order to discover good representations, often at
multiple levels, with higher-level
learned features deﬁned in terms of lower-level features. The objective
is to make these
higher-level representations more abstract, with their individual
features more invariant to most
of the variations that are typically present in the training
distribution, while collectively
preserving as much as possible of the information in the input.
Ideally, we would like these
representations to disentangle the unknown factors of variation that
underlie the training
distribution. Such unsupervised learning of representations can be
exploited usefully under
the hypothesis that the input distribution <span class="cmmi-10x-x-109">P</span>(<span
 class="cmmi-10x-x-109">x</span>) is structurally related to some task
of
interest, say predicting <span class="cmmi-10x-x-109">P</span>(<span
 class="cmmi-10x-x-109">y</span><span class="cmsy-10x-x-109">|</span><span
 class="cmmi-10x-x-109">x</span>). This paper focuses on the context of
the Unsupervised and
Transfer Learning Challenge, on why unsupervised pre-training of
representations can be
useful, and how it can be exploited in the transfer learning scenario,
where we care
about predictions on examples that are not from the same distribution
as the training
distribution.
</div>
<hr></div>
<!--#include virtual="/nav-bar.txt"-->
</body>
</html>
