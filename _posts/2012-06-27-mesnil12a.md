---
pdf: http://proceedings.mlr.press/v27/mesnil12a/mesnil12a.pdf
section: challenge
title: 'Unsupervised and Transfer Learning Challenge: a Deep Learning Approach'
abstract: Learning good representations from a large set of unlabeled data is a particularly
  challenging task. Recent work (see ? for a review) shows that training deep architectures
  is a good way to extract such representations, by extracting and disentangling gradually
  higher-level factors of variation characterizing the input distribution. In this
  paper, we describe different kinds of layers we trained for learning representations
  in the setting of the Unsupervised and Transfer Learning Challenge. The strategy
  of our team won the final phase of the challenge. It combined and stacked different
  one-layer unsupervised learning algorithms, adapted to each of the five datasets
  of the competition. This paper describes that strategy and the particular one-layer
  learning algorithms feeding a simple linear classifier with a tiny number of labeled
  training samples (1 to 64 per class).
layout: inproceedings
id: mesnil12a
month: 0
firstpage: 97
lastpage: 110
page: 97-110
sections: 
author:
- given: Gr√©goire Mesnil Yann
  family: Dauphin
- given: Xavier
  family: Glorot
- given: Salah
  family: Rifai1
- given: Yoshua
  family: Bengio
- given: Ian
  family: Goodfellow
- given: Erick
  family: Lavoie
- given: Xavier
  family: Muller
- given: Guillaume
  family: Desjardins
- given: David
  family: Warde-Farley
- given: Pascal
  family: Vincent
- given: Aaron
  family: Courville
- given: James
  family: Bergstra
date: 2012-06-27
address: Bellevue, Washington, USA
publisher: PMLR
container-title: Proceedings of ICML Workshop on Unsupervised and Transfer Learning
volume: '27'
genre: inproceedings
issued:
  date-parts:
  - 2012
  - 6
  - 27
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
